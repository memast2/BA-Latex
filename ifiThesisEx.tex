\documentclass[abstracton,12pt]{scrreprt}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{times}
\usepackage{listings}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{url}
\usepackage{chapterbib}
\usepackage{gensymb}



% --------- 

\titlehead{Department of Informatics, University of Zürich}
\subject{\vspace*{2cm}BSc Thesis}
\title{Implementing an Index Structure for Streaming Time Series Data}
\author{
  Melina Mast\\[-5pt]
  \scriptsize Matrikelnummer: 13-762-588\\[-5pt]
  \scriptsize Email: \texttt{melina.mast@uzh.ch}
}
\date{\vspace*{2cm}August, 2016}
\publishers{
  \small supervised by Prof.\ Dr.\ Michael Böhlen and Kevin Wellenzohn \\[5cm]
  \begin{tikzpicture}[overlay]
    \node at (-3,-3) {\includegraphics[height=5.5cm]{IFIlogo}};
    \node at (7,-3) {\includegraphics[height=2cm]{dbtgBW}};
  \end{tikzpicture}
}

%----\dedication{dedicated to xxx}

% --------- 

\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}

\newenvironment{proof}
  {\noindent{\bf Proof:\rm}}{\hfill$\Box$\vspace{\medskipamount}}

\def\bbbr{{\rm I\!R}}
\def\bbbm{{\rm I\!M}}
\def\bbbn{{\rm I\!N}}
\def\bbbz{{\rm I\!Z}}

% --------- 

\begin{document}

\maketitle

\chapter*{Acknowledgements}

I especially would like to thank my supervisor Kevin Wellenzohn, who has supported me with guidance and constructive feedback during my work. Besides, I would like to thank Prof. Dr. Michael Böhlen for the opportunity to write my bachelor thesis at the Database Technology Group of the University of Zurich. ...   

\begin{abstract}
  ...
\end{abstract}

\chapter*{Zusammenfassung}

\tableofcontents
\listoffigures
\listoftables
\listofalgorithms


\chapter{Introduction}
A streaming time series \(s\) is a unbounded sequence of data points that is continuously extended, potentially forever. They are relevant to applications in diverse domains like in finance, meteorology or sensor networks to name a few. All these applications need to be fed continuously with the latest data. But the processing of large volumes of time series data is impractical because a system can only keep a limited size of data in main memory.\\
Since streaming time series are unbounded, a system cannot hold the constantly generated data in his main memory endlessly. Therefore, the data that is kept in main memory needs to be limited to just a portion of the streaming time series. Besides, in order to be practical for a application like the financial stock market, the data that arrives in a defined time interval (e.g. every 2 minutes) needs to be completely processed until the succeeding data arises.\\
The thesis presents a way to implement the described data structures after discussing the requirements. Furthermore, it documents the out coming experimental results.
In the end of the thesis, in Chapter \ref{sec:Summary}, the findings will be summarized and concluded.

\chapter{Background and Related Work}
Time series data is not always gapless. E.g. due to sensor failures or transmission errors values can get missing. But since many applications need complete data before further data processing is possible, the missing values need to be recovered first. \\The paper \emph{Continuous Imputation of Missing Values in Highly Correlated Streams of Time Series Data} \cite{BScT} presents a query pattern over the most recent values of a set of time series. The algorithm was developed in the context of meteorology. The \emph{Südtiroler Beratungsring (SBR)} operates a network of weather stations where each station collects continuously (every five minutes) new temperature data. If a value is missing due to sensor failures or other problems, it will be continuously imputed. The imputation needs to fulfill two main requirements: \\
\begin{itemize}  
	\item The algorithm needs to be efficient enough to complete the data imputation before the new measurement arrives. 
	\item The imputation must rely on past measurements.\\
\end{itemize}
The algorithm provided in the paper is called Top-\emph{k} Case Matching (TKCM) algorithm. It defines a two-dimensional query pattern that contains the measurements of the reference time series in a window of time. It seeks for the \emph{k} patterns that match the query pattern best. The missing value is derived from the \emph{k} past pattern. Therefore, it determines for each \emph{time series} a small set of highly correlated \emph{reference time series}. To scan the entire data set to identify the \emph{k} best is not practical. Hence, only a small portion of the data is scanned. In the following, the data set of the \emph{SBM} is described in Section \ref{sec:DS}. After, the TKCM algorithm is introduced in Section \ref{sec:TKCM}.

\label{sec:DS}
\section{The Data Set}
The \emph{SBR} operates 115 weather stations in South Tyrol. Each of the weather stations records temperature every five minutes. 8\% of the data set is missing. Missing values often occur in groups either due to transmission problems which lead to smaller missing groups of no more than 5 consecutive missing values or sensor failures which may lead to larger missing groups. 87\% of the missing blocks are small and therefore not more than 5 blocks in length. The algorithm is described in Section \ref{sec:TKCM}. 

\label{sec:TKCM}
\section{TKCM}
The implementation of the index structure described in the following thesis is a fundamental part of the TKCM algorithm.\\
The algorithm is a \emph{k}-Nearest Neighbor Imputation algorithm that uses reference time series to find similar weather situations in the past. \\Let $W=[ \underline{t}, \bar{t} ]$ be a sliding window of length $|W|$. Time $\underline{t}$ stands for the oldest time point that fits into the time window and $\bar{t}$ stands for the current time point for which the stream produced a new value. Besides, consider a set $S = \{s_1,s_2,...\}$ of streaming time series. The value of time series \emph{s} e S at time \emph{t} is denoted as $s(t)$. Only the values in the time window $W$ are kept in main memory. If the current value of the time series is missing we write $s(\bar{t})=NIL$. However, all the time points $t < \bar{t}$ have a time series \emph{s} that is complete. Hence, $\forall t < \bar{t} : s(t) \ne NIL$ since \emph{s} contains imputed values if the real ones were missing. To impute the missing values, reference time series are used. Let $R_s = [r_1, r_2, ...]$ be a sequence of \emph{reference time series} $r_i \in S \setminus \{s\}$ for time series \emph{s}. TKCM chooses for each base time series $s \in S$ the ranked sequence of reference time series $R_s$. The fundamental idea is to search for a co-occurrence for two time points $t$, $\bar{t}$ where the reference time series \emph{r} has values $r(\bar{t})\approx r(t)$, then also $s(\bar{t}) \approx s(t)$. Parameter \emph{l} stands for the number of reference time series that TKCM should consider. $R_s$ is computed using the Case Matching Algorithm further described in the paper. It consists of \emph{l} reference values at time $\bar{t}$. Further, we define $R_s^l$ as the first \emph{l} time series of $R_s$ that have a value $r_i(\bar{t}) \ne NIL$. \\
The two-dimensional query pattern $P_{\bar{t}}$ is defined with \emph{l} reference time series on the spatial dimension and a time window of length \emph{p} on the time dimension.
\newtheorem{defn}{Definition}[section]
\begin{defn}
	Query-Pattern. Let p and o denote the pattern length and pattern offset, respectively, where $p > 1$ and $0 \leq o < p$. Query pattern $P_{\bar{t}}$ contains all values of time series $R_s^l \cup \{s\}$ in the window of time $[\bar{t} - p, \bar{t}]$, except the missing value $s(\bar{t}) = NIL$. $P_{\bar{t}}$ is a set of triples $(r,o,r( \bar{t}-o))$, such that:\\\\
	$P_{\bar{t}}=\{(r,o,r( \bar{t}-o))| r \in \{s\} \cup R_s^l \land 0 \cup 0 \leq o < p\} \ne \{(s,0,s(\bar{t}))\}$\\\\
	The value of a cell is given by $P_{\bar{t}}^{r,o}$, hence $(r, o, P_{\bar{t}}^{r,o}) \in P_{\bar{t}}$.
\end{defn}
\newtheorem{exmp}{Example}[section]
\begin{exmp}
A data example for a triple is $(r_1,0,20.5^{\circ}C)$, where $r_1$ represents the reference values, $0$ is the pattern offset \emph{o} and $20.5^{\circ}C$ represents the degree at time $\bar{t}$. A pattern example for l = 2 and p = 1 would be: $P_{\bar{t}}=\{(r_1,0,20.5^{\circ}C),(r_2,0,19.5^{\circ}C)\}$.
\end{exmp}
The TKCM algorithm searches for the \emph{k} patterns $P_t$ in the time window $W$ that best match the query pattern $P_{\bar{t}}$. The pattern $P_t$ matches the query patter $P_{\bar{t}}$ if for each cell $P_{\bar{t}}^{r,o} \in P_{\bar{t}}$ a corresponding cell  $P_{t}^{r,o} \in P_t$ exists. The error between a pattern $P_t$ and $P_{\bar{t}}$  is the sum of the cell-wise differences in the patterns:\\\\
$\delta(P_{t}, P_{\bar{t}}) = \displaystyle\sum_{P_{\bar{t}}^{r,o} \in P_{\bar{t}}}^{}$ $|P_{t}^{r,o} - P_{\bar{t}}^{r,o}|$\\
The \emph{k} patterns that minimize $\delta(P_{t}, P_{\bar{t}})$ are matched by the TKCM. Further, their time points \emph{t} is collected in a set $T_k$:\\\\
\newcommand*{\argmin}{\operatornamewithlimits{argmin}\limits}
$T_i = T_{i-1} \cup \{\argmin_{%
	\substack{%
		\text t \in W \\
		\text t \neq \bar{t} \wedge t \notin T_{i-1}
	}
}	\delta(P_{t}, P_{\bar{t}})\}$\\\\
The matched time points $T_k$ are used for the imputation. The value $\hat{s}(t)$ that is calculated as the average of the values $\{s(t) | t \in T_k\}$ will be imputed. The calculation is as follows: \\\\
$\hat{s}(t) = \frac{1}{|T_k|} \displaystyle\sum_{t \in T_k}^{} s(t)$

\begin{exmp}
	A data example for a triple is $(r_1,0,20.5^{\circ}C)$, where $r_1$ represents the reference values, $0$ is the pattern offset \emph{o} and $20.5^{\circ}C$ represents the degree at time $\bar{t}$. A pattern example for l = 2 and p = 1 would be: $P_{\bar{t}}=\{(r_1,0,20.5^{\circ}C),(r_2,0,19.5^{\circ}C)\}$.
\end{exmp}



\label{sec:ImpTKCM}
\section{Connection with the Thesis}
\subsection{Implementing TKCM}


\chapter{Preliminaries}
\label{sec:P}


\section{Handling Duplicate Values}

B+ trees are often 

\subsection{Add the time to the key}
If you would add the timestamps to keys would be unique. So if the case occurs where you have the same keys but in different leaves because the leaves were full and had therefore be splitted you could include the timestamps which divides the two different values to the parent. --> every key with timestamp smaller than the parents timestamp is in the left leaf and every key with timestamp bigger or equal is in the right one --> 
Problems: more memory needed. always check needed if there is a timestamps in the leaf
+ neighbour method would still work because you have the value and the set of time stamps 

\subsection{Add several times to the key}

The second was to have just one entry for each key, but the 'payload' associated with each key points to a different block of data, which is a linked list pointing to all instances of items that are having the same key


\subsection{Not change insertion but the search algorithm}
There are several different ways of handling duplicate keys. One way is use an unmodified insert algorithm which allows duplicate keys in blocks but is otherwise unchanged. The issue with a structure such as this is the search algorithm must be modified to take into account several corner cases which arise
For instance one of the invariants of a B+Tree may be violated in this structure. Specifically if there are many duplicate keys, a copy of one of the keys may be in a non-leaf block. However, the key may appear in blocks that which appear logically before the block which is pointed at by the key in the internal block. Thus the search algorithm must be modified to look in the previous blocks to the one suggested by the unmodified search algorithm. This will slow down the common case for search.There is another issue with this straight forward implementation, if there are many duplicate keys in the index, the index size may be taller than necessary. Consider a situation were for each unique key there are perhaps hundreds of duplicates, the index size will be proportional to the total number of keys in the main file, however, you only need to index an index on the unique keys. One of the files indexed in our program will be indexing has such characteristics to its data. It indexes strings (as the keys) with associated instances where those strings show up in our documents. There can be hundreds to thousands of instances of each unique string.
Therefore the approach I took was to store only the unique keys in the index, and have the duplicates captured in overflow blocks in the main file. An example of such a tree can be seen in figure 2. Consider key 6; there are 5 instances of this key in the tree. The tree is order 3, indicating the keys cannot all fit in one block. To handle this situation an overflow block is chained to the block which is indexed by the tree structure. The overflow block then points to the next relevant block in the tree.To create a structure such as this, the insert algorithm had to be modified. Like the previous version these modifications do not come without a cost, in particular the invariant which states all block must be at least half full has been relaxed. This is not true in this B+Tree, some blocks like the one containing key number 7, are not half full. This problem could be partially solved by using key rotations to balance the tree better. However, there are still corner cases where there would be a block which is under-full. One such corner case includes when a key falls between two keys which have overflow blocks. It must then be in a block by itself, since this B+Tree has the invariant which state that if a block is overflowing it can only contain one unique key. In the future we would like to implement key rotations to help partially alleviate the problem of under-full blocks.

The advantage of this approach to B+Trees with duplicate keys is the index size is small no matter how the ratio of duplicate keys to the total number of keys in the file. This property allows our searches to be conducted quicker. Since the overflow blocks are chained into the B+Tree structure we still have the property of being able to fast sequential scans. One consequence is we have defined all queries on our B+Trees to be range queries. This is fine because all of our queries were already range queries. In conclusion we relax the condition that all blocks must be at least half full to gain higher performance during search.

http://hackthology.com/lessons-learned-while-implementing-a-btree.html

\chapter{Implementation}
\label{sec:Implementation}


Let $W=[ \underline{t}, \bar{t} ]$ be a sliding window of length $|W|$. Time $\underline{t}$ stands for the oldest time point that fits into the time window and $\bar{t}$ stands for the newest time point for which the stream produced a new value. \\\\The system uses two different data structures: a circular array and a $B+$ tree. needs to be efficiently perform the following operations on the streaming time series $s$ in a sliding window $|W|$: \\
\begin{itemize}  
	\item shift$(\bar{t}, v)$: add value \emph{v} for the new current time point $\bar{t}$ and remove value \emph{v'} for the time point $\underline{t} - 1$ that just dropped out of time window $W$.
	\item lookup$(t)$: return the value of time series \emph{s} at time \emph{t}, denoted by $s(t)$.
	\item neighbour$(v, T)$: given a value \emph{v} and a set of time points $T$, return the time point $t e T$ such that $|v-s(t)|$ is minimal.\\
\end{itemize}


\chapter{Experimental Evaluation}
\label{sec:Experimental}
\section{Experimental Setup}
\section{Results}
\section{Discussion}

\chapter{Summary and Conclusion}
\label{sec:Summary}
"The conclusion (10 to 12 per cent of the whole research thesis) does not only summarize the whole research thesis, but it also evaluates the results of the scientific inquiry. Do the results confirm or reject previously formulated hypotheses? The conclusion draws both theoretical and practical lessons that could be used in future analyses. These lessons are to be embedded as
2
recommendations for the research community and for policy-makers (note: policy relevance instead of policy prescriptive). In addition, the conclusion gives insights for further research."

\begin{thebibliography}{99}
	\bibliographystyle{alpha}
	
	\bibitem{BScT} K. Wellenzohn, M. Böhlen, A. Dignos, J. Gamper, and H. Mitterer: \emph{Continuous imputation of missing values in highly correlated streams of time series data}; Unpublished, 2016.
	
	\bibitem{OnlineAmnesicAppr} Themistoklis Palapanas, Michail Vlachos, Eamonn Keogh, Dimitrios Gunopulos, Wagner Truppel: \emph{Online Amnesic Approximation of Streaming Time Series}; University of California, Riverside, USA, 2004. \url{http://www.cs.ucr.edu/~eamonn/ICDM_2004.pdf}
	
	\bibitem{AuthentificationMethod} Xiaomei Dong, Xiaohua Li: \emph{An Authentification Method for Self Nodes Based on Watermarking in Wireless Sensor Networks}; Northeastern University, Liaoning, China, 2009.
	\bibitem{AuthentificationMethod} Xiaomei Dong, Xiaohua Li: \emph{An Authentification Method for Self Nodes Based on Watermarking in Wireless Sensor Networks}; Northeastern University, Liaoning, China, 2009.
	\bibitem{SecInWSN:IssChall} All-Sakib Khan Pathan, Hyung-Woo Lee, Choong Seon Hong: \emph{Security in Wireless Sensor Networks: Issues and Challenges}; ISBN 89-5519-129-4, DOI: 10.1109/ICACT.2006.206151, Februar 2006. \url{http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1625756&isnumber=34121}
		
	



\end{thebibliography}






\end{document}
