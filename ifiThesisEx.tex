\documentclass[abstracton,12pt]{scrreprt}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{times}
\usepackage{listings}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{url}
\usepackage{chapterbib}
\usepackage{gensymb}
\usepackage{BTree}
\usepackage{weiwBTree}
\usepackage{float}
\usepackage{array}

\usetikzlibrary{shapes, calc}




% --------- 

\titlehead{Department of Informatics, University of Zürich}
\subject{\vspace*{2cm}BSc Thesis}
\title{Implementing an Index Structure for Streaming Time Series Data}
\author{
  Melina Mast\\[-5pt]
  \scriptsize Matrikelnummer: 13-762-588\\[-5pt]
  \scriptsize Email: \texttt{melina.mast@uzh.ch}
}
\date{\vspace*{2cm}August, 2016}
\publishers{
  \small supervised by Prof.\ Dr.\ Michael Böhlen and Kevin Wellenzohn \\[5cm]
  \begin{tikzpicture}[overlay]
    \node at (-3,-3) {\includegraphics[height=5.5cm]{IFIlogo}};
    \node at (7,-3) {\includegraphics[height=2cm]{dbtgBW}};
  \end{tikzpicture}
}

%----\dedication{dedicated to xxx}

% --------- 

\newtheorem{definition}{Definition}
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}

\newenvironment{proof}
  {\noindent{\bf Proof:\rm}}{\hfill$\Box$\vspace{\medskipamount}}

\def\bbbr{{\rm I\!R}}
\def\bbbm{{\rm I\!M}}
\def\bbbn{{\rm I\!N}}
\def\bbbz{{\rm I\!Z}}

% --------- 

\begin{document}

\maketitle

\chapter*{Acknowledgements}



\begin{abstract}
  ...
\end{abstract}

\chapter*{Zusammenfassung}

\tableofcontents
\listoffigures
\listoftables
\listofalgorithms


\chapter{Introduction}
A streaming time series \(s\) is a unbounded sequence of data points that is continuously extended, potentially forever. They are relevant to applications in diverse domains like in finance, meteorology or sensor networks to name a few. All these applications need to be fed continuously with the latest data. But the processing of large volumes of time series data is impractical because a system can only keep a limited size of data in main memory.\\
Since streaming time series are unbounded, a system cannot hold the constantly generated data in his main memory endlessly. Therefore, the data that is kept in main memory needs to be limited to just a portion of the streaming time series. Besides, in order to be practical for a application like the financial stock market, the data that arrives in a defined time interval (e.g. every 2 minutes) needs to be completely processed until the succeeding data arises.\\
The thesis presents a way to implement the described data structures after discussing the requirements. Furthermore, it documents the out coming experimental results.
In the end of the thesis, in Chapter \ref{sec:Summary}, the findings will be summarized and concluded.

\section{Thesis Outline}
...First...

\chapter{Background and Related Work}
Time series data is not always gapless. E.g. due to sensor failures or transmission errors values can get missing. But since many applications need complete data before further data processing is possible, the missing values need to be recovered first. \\The paper \emph{Continuous Imputation of Missing Values in Highly Correlated Streams of Time Series Data} \cite{BScT} presents a two-dimensional query pattern over the most recent values of a set of time series. The algorithm was developed in the context of meteorology. The \emph{Südtiroler Beratungsring (SBR)} operates a network of weather stations where each station collects continuously (every five minutes) new temperature data. If a value is missing due to sensor failures or other problems, it will be continuously imputed. The imputation needs to fulfill two main requirements: \\
\begin{itemize}  
	\item The algorithm needs to be efficient enough to complete the data imputation before the new measurement arrives. 
	\item The imputation must rely on past measurements.\\
\end{itemize}
The algorithm provided in the paper is called Top-\emph{k} Case Matching (TKCM) algorithm. It defines a two-dimensional query pattern that contains the measurements of the reference time series in a window of time. It seeks for the \emph{k} patterns that match the query pattern best. The missing value is derived from the \emph{k} past pattern. Therefore, it determines for each \emph{time series} a small set of highly correlated \emph{reference time series}. To scan the entire data set to identify the \emph{k} best is not practical. Hence, only a small portion of the data is scanned. In the following, the data set of the \emph{SBM} is described in Section \ref{sec:DS}. After, the TKCM algorithm presented in \cite{BScT} is introduced. Finally, the connection to the present thesis is made in Section \ref{sec:ImpTKCM} and the implementation is introduced. 


\section{The Data Set}
\label{sec:DS}
The \emph{SBR} operates 115 weather stations in South Tyrol. Each of the weather stations records temperature every five minutes. 8\% of the data set is missing. Missing values often occur in groups either due to transmission problems which lead to smaller missing groups of no more than 5 consecutive missing values or sensor failures which may lead to larger missing groups. 87\% of the missing blocks are small and therefore not more than 5 blocks in length. The algorithm is described in Section \ref{sec:TKCM}. 


\section{TKCM}
\label{sec:TKCM}
The implementation of the index structure described in the following thesis is a fundamental part of the TKCM algorithm.\\
The algorithm is a \emph{k}-Nearest Neighbor Imputation algorithm that uses reference time series to find similar weather situations in the past. \\Let $W=[ \underline{t}, \bar{t} ]$ be a sliding window of length $|W|$. Time $\underline{t}$ stands for the oldest time point that fits into the time window and $\bar{t}$ stands for the current time point for which the stream produced a new value. Besides, consider a set $S = \{s_1,s_2,...\}$ of streaming time series. The value of time series \emph{s} e S at time \emph{t} is denoted as $s(t)$. Only the values in the time window $W$ are kept in main memory. If the current value of the time series is missing we write $s(\bar{t})=NIL$. However, all the time points $t < \bar{t}$ have a time series \emph{s} that is complete. Hence, $\forall t < \bar{t} : s(t) \ne NIL$ since \emph{s} contains imputed values if the real ones were missing. To impute the missing values, reference time series are used. Let $R_s = [r_1, r_2, ...]$ be a sequence of \emph{reference time series} $r_i \in S \setminus \{s\}$ for time series \emph{s}. TKCM chooses for each base time series $s \in S$ the ranked sequence of reference time series $R_s$. The fundamental idea is to search for a co-occurrence for two time points $t$, $\bar{t}$ where the reference time series \emph{r} has values $r(\bar{t})\approx r(t)$, then also $s(\bar{t}) \approx s(t)$. Parameter \emph{l} stands for the number of reference time series that TKCM should consider. $R_s$ is computed using the Case Matching Algorithm further described in the paper. It consists of \emph{l} reference values at time $\bar{t}$. Further, we define $R_s^l$ as the first \emph{l} time series of $R_s$ that have a value $r_i(\bar{t}) \ne NIL$. \\
The two-dimensional query pattern $P_{\bar{t}}$ is defined with \emph{l} reference time series on the spatial dimension and a time window of length \emph{p} on the time dimension.
\newtheorem{defn}{Definition}[section]
\begin{defn}
	Query-Pattern. Let p and o denote the pattern length and pattern offset, respectively, where $p > 1$ and $0 \leq o < p$. Query pattern $P_{\bar{t}}$ contains all values of time series $R_s^l \cup \{s\}$ in the window of time $[\bar{t} - p, \bar{t}]$, except the missing value $s(\bar{t}) = NIL$. $P_{\bar{t}}$ is a set of triples $(r,o,r( \bar{t}-o))$, such that:
	\begin{align*}
P_{\bar{t}}=\{(r,o,r( \bar{t}-o))| r \in \{s\} \cup R_s^l \land 0 \cup 0 \leq o < p\} \ne \{(s,0,s(\bar{t}))\}
	\end{align*}
	The value of a cell is given by $P_{\bar{t}}^{r,o}$, hence $(r, o, P_{\bar{t}}^{r,o}) \in P_{\bar{t}}$.
\end{defn}
\newtheorem{exmp}{Example}[section]
\begin{exmp}
A data example for a triple is $(r_1,0,20.5^{\circ}C)$, where $r_1$ represents the reference values, $0$ is the pattern offset \emph{o} and $20.5^{\circ}C$ represents the degree at time $\bar{t}$. A pattern example for l = 2 and p = 1 would be: $P_{\bar{t}}=\{(r_1,0,20.5^{\circ}C),(r_2,0,19.5^{\circ}C)\}$.
\end{exmp}
The TKCM algorithm searches for the \emph{k} patterns $P_t$ in the time window $W$ that best match the query pattern $P_{\bar{t}}$. The pattern $P_t$ matches the query patter $P_{\bar{t}}$ if for each cell $P_{\bar{t}}^{r,o} \in P_{\bar{t}}$ a corresponding cell  $P_{t}^{r,o} \in P_t$ exists. The error between a pattern $P_t$ and $P_{\bar{t}}$  is the sum of the cell-wise differences in the patterns:
\begin{align*}
\delta(P_{t}, P_{\bar{t}}) = \displaystyle\sum_{P_{\bar{t}}^{r,o} \in P_{\bar{t}}}^{} |P_{t}^{r,o} - P_{\bar{t}}^{r,o}|
\end{align*}
The \emph{k} patterns that minimize $\delta(P_{t}, P_{\bar{t}})$ are matched by the TKCM. Further, their time points \emph{t} is collected in a set $T_k$:
\newcommand*{\argmin}{\operatornamewithlimits{argmin}\limits}
\begin{align*}
T_i = T_{i-1} \cup \{\argmin_{%
	\substack{%
		\text t \in W \\
		\text t \neq \bar{t} \wedge t \notin T_{i-1}
	}
}	\delta(P_{t}, P_{\bar{t}})\}
\end{align*}
The matched time points $T_k$ are used for the imputation. The value $\hat{s}(t)$ that is calculated as the average of the values $\{s(t) | t \in T_k\}$ will be imputed. The calculation is as follows: 
\begin{align*}
\hat{s}(t) = \frac{1}{|T_k|} \displaystyle\sum_{t \in T_k}^{} s(t)
\end{align*}
\begin{exmp}
	If the base time series s has a missing value at time $\bar{t}=20:20$ the value needs to be imputed. The query pattern $P_{\bar{t}}$ is defined in this example with $l=2$ reference time series and pattern length $p=2$. Assumed that the $k=3$ most similar matches are $P_{20:15} = 19.5^{\circ}C$, $P_{20:10}= 19.1^{\circ}C$ and $P_{19:40} = 19.9^{\circ}C$, the computation for $\hat{s}(t)$ is as follows: $\hat{s}(t) = \frac{1}{3}(19.5^{\circ}C + 19.1^{\circ}C + 19.9^{\circ}C) = 19.5^{\circ}C$. Hence, $19.5^{\circ}C$ is imputed for the missing value at time $\bar{t}=20:20$.
\end{exmp}
In conclusion, TKCM is able to calculate an estimation of a missing value in streaming time series data. To achieve this, TKCM uses three parameters, \emph{k, l} and \emph{p}. Parameter \emph{k} represent the number of patterns TKCM matches and collects. In practice $k =[1,40]$ most useful, since the impact of an exceptional outlier is less aggravated. The parameters \emph{l} and \emph{p} define the size of the two dimensional query pattern $P_{\bar{t}}$. The two dimensions are \emph{l} for space and \emph{p} for time. The accuracy of the imputation value increases as both parameters \emph{l,p} increase, because more relevant query patterns can be matched. But the runtime of TKCM increases with large query patterns. Since the missing values often occur in blocks, as described in Section \ref{sec:DS}, the parameters \emph{l} and \emph{p} generally can be set moderately. E.g. 
$l \in [3,5]$ and $p \in [2,6]$ because most of the missing blocks are no more than 5 consecutive values. If 
$p \in [2,6]$, the query pattern $P_{\bar{t}}$ contains at least one past value of the base time series \emph{s} in 87\% of the cases.


\section{Implementing TKCM}
\label{sec:ImpTKCM}
The TKCM must not only insert missing values, but also process the newest arriving values efficiently. Therefore, TKCM must provide an insertion method for new arriving values to insert the new value into the time window $W$. Since the time window has a limited size $|W|$, a old value has to be deleted for the new arriving value. Provided that, the oldest time \emph{t} does not fit in the time window any more if the window is already full. Besides, the most similar base time values for a given value \emph{v} should be efficiently found and returned. \\
The implementation of these requirements is the main topic of the present thesis. Therefore, the implementation of the TKCM is introduced in the following.

\subsection{Methods Definition}
Retrieving the \emph{k} most similar patterns to a query pattern $P_{\bar{t}}$ is the computationally most expensive part of TKCM. \\
TKCM initializes a set $T =\{\}$. The set is filled during execution with all time points \emph{t} for which pattern $P_t$ has already been compared to the query pattern $P_{\bar{t}}$. Besides, TKCM initializes a set $T^*=\{\}$ that contains the \emph{k} time points $t \in T$ that minimize the error $\delta(P_{t}, P_{\bar{t}})$. Therefore, $T^* \subseteq T$ is always true during execution. 
\\TKCM uses two methods for accessing any time series $r \in S$, \emph{random} and \emph{sorted} access. The two methods are defined as follows: 
\begin{defn}
	Random Access. Random access returns value r(t), given time series r and time point t.	
\end{defn}
\begin{defn}
Sorted Access. Sorted access returns the next yet unseen time point $t_s \notin T$ such that the value $r(t_s-o)$ is most similar to a given pattern cell $P_{\bar{t}}^{r,o}$. $t(s)$ is defined as:
\begin{align*}
t_s = \argmin_{t_s \in W \setminus T} |r(t_s-o) - P_{\bar{t}}^{r,o}|
\end{align*}
\end{defn}
After $T$ and $T^*$ is initialized, TKCM iterates until set $T^*$ contains the $k$ time points $t$ that minimize the difference  $\delta(P_{t}, P_{\bar{t}})$. \\Using the sorted access mode, the algorithm loops through the cells $P_{\bar{t}}^{r,o}$, reading the next potential time point $t_s \notin T$. The time point $t_s \notin T$ is added to $T$. The time point $t_s$ has a corresponding patter $P_{t_s}$ which is at least for one pattern cell similar to the query pattern $P_{\bar{t}}$. \\
The random access mode is used to look up the values that pattern $P_{t_s}$ is composed of. After each iteration a threshold $\tau$ is computed. The threshold $\tau$ is a lower-bound on the error $\delta (P_{t^{'}}, P_{\bar{t}})$ for any time point $t^{'}$ that is yet unseen. Therefore, during the execution of the algorithm
$\forall t^{'} \in T : \tau \leq \delta(P_{t^{'}}, P_{\bar{t}}) $ is valid. Informally this significances that the lower-bound is always smaller or equal to the error between pattern $P_{t^{'}}$ and query patter $P_{\bar{t}}$ for all time points $t^{'}$ that are elements of $T$. Once $\forall t \in T^* : \delta(P_{t}, P_{\bar{t}}) \leq \tau$ the algorithm terminates. At the end, $T^*=T_k$. 

\subsection{Data Structures}
The above described access modes can be efficiently performed by combining two data structures. Namely, a $B^+$ tree and a circular array. Each time series $s \in S$ can be implemented as a circular array. The circular array is kept in main memory. It uses random access to look up value $s(t)$ for a given time $t$. Further, TKCM maintains for each time series $s$ a $B^+$ tree that is also kept in main memory. The $B^+$ tree is ideal for sorted access by value and therefore for range queries. Unlike the well known $B^+$tree, where the leaves are just linked to their succeeding leaf, like shown in Figure \ref{fig:tratree}, the leaves are linked to the succeeding as well as the preceding leaf as shown in Figure \ref{fig:sptree}.

\begin{figure}[H]
		\centering
		\begin{tikzpicture}[
		scale=0.6,
		every node/.style={outer sep=0pt, transform shape, font=\scriptsize},
		every matrix/.style={cells={scale=0.6}},
		]

		\btreeinodefourx{root}{13}{17}{24}{30};
		\xyshiftx{-50mm}{-20mm}{\btreelnodefourx{n1}{2}{3}{5}{7}}
		\xyshiftx{0mm}{-20mm}{\btreelnodefourx{n2}{14}{16}{}{}}
		\xyshiftx{50mm}{-20mm}{\btreelnodefourx{n3}{19}{20}{22}{}}
		\xyshiftx{100mm}{-20mm}{\btreelnodefourx{n4}{24}{27}{29}{}}
		\xyshiftx{150mm}{-20mm}{\btreelnodefourx{n5}{33}{34}{38}{39}}
		% 
		\foreach \x in {1,2,...,5} { \btreelinkx{root-\x}{n\x} }
		{\linkleavesOnex{n1}{n2} }
		{\linkleavesOnex{n2}{n3} }
		{\linkleavesOnex{n3}{n4} }
		{\linkleavesOnex{n4}{n5} }
		\end{tikzpicture}

	\caption{Traditional $B^+$ tree.}
	\label{fig:tratree}
\end{figure}


\begin{figure}[H]
	\centering
	\begin{tikzpicture}[
	scale=0.6,
	every node/.style={outer sep=0pt, transform shape, font=\scriptsize},
	every matrix/.style={cells={scale=0.6}},
	]
		% 
		\btreeinodefourx{root}{24}{}{}{};
		\xyshiftx{-40mm}{-20mm}{\btreeinodefourx{n1}{13}{17}{}{}}
		\xyshiftx{70mm}{-20mm}{\btreeinodefourx{n2}{30}{38}{}{}}
		% 
		\xyshiftx{-100mm}{-40mm}{\btreelnodefourx{n11}{2}{3}{5}{7}}
		\xyshiftx{-55mm}{-40mm}{\btreelnodefourx{n12}{14}{16}{}{}}
		\xyshiftx{-10mm}{-40mm}{\btreelnodefourx{n13}{19}{20}{22}{}}
		\xyshiftx{35mm}{-40mm}{\btreelnodefourx{n21}{24}{27}{29}{}}
		\xyshiftx{80mm}{-40mm}{\btreelnodefourx{n22}{33}{34}{}{}}
		\xyshiftx{125mm}{-40mm}{\btreelnodefourx{n23}{38}{39}{40}{}}
		% 
		\foreach \x in {1,2} { \btreelinkx{root-\x}{n\x} }
		\foreach \x in {1,2,3} { \btreelinkx{n1-\x}{n1\x} }
		\foreach \x in {1,2,3} { \btreelinkx{n2-\x}{n2\x} }
		{\linkleavesx{n11}{n12} }
		{\linkleavesx{n12}{n13} }
		{\linkleavesx{n13}{n21} }
		{\linkleavesx{n21}{n22} }
		{\linkleavesx{n22}{n23} }
		\end{tikzpicture}
	\caption{A $B^+$ tree with its leaves linked to the preceding and succeeding leaf.}
	\label{fig:sptree}
\end{figure}



The implementation of the data structures, as well as the implementation of sorted access and random access, is topic of the present thesis. The requirements and the used data structures are discussed in Chapter \ref{sec:P}. In addition to that, the implementation is described in Chapter \ref{sec:Implementation}, and finally the implementation is evaluated and its experimental results are discussed in Chapter \ref{sec:Experimental}.  


\chapter{Requirements}
\label{sec:P}
This chapter the requirements that the later described implementation of the index structure must meet are described, in order to be useful for the TKCM algorithm. The operations are introduced in Section \ref{sec:Op}. Further, the data structures are described in Section \ref{sec:das} and finally the ...

\section{Operations}
\label{sec:Op}
A streaming time series $s$ is a sequence of data points that is extended continuously for example every 5 minutes and eventually forever. Since streaming time series are unbounded, a system can only keep a portion of a time series in main memory. \cite{BScT} proposes an index structure that meet the requirements for streaming time series data.\\
Let $W=[ \underline{t}, \bar{t} ]$ be a sliding window of length $|W|$. Time $\underline{t}$ stands for the oldest time point that fits into the time window and $\bar{t}$ stands for the newest time point for which the stream produced a new value. \\\\
The system presented in the present thesis that uses the proposed index structure in \cite{BScT}, namely a circular array and a $B+$ tree, needs to efficiently perform on the streaming time series $s$ in a sliding window $|W|$: \\
\begin{itemize}  
	\item shift$(\bar{t}, v)$: add value \emph{v} for the new current time point $\bar{t}$ and remove value \emph{v'} for the time point $\underline{t} - 1$ that just dropped out of time window $W$.
	\item lookup$(t)$: return the value of time series \emph{s} at time \emph{t}, denoted by $s(t)$.
	\item neighbor$(v, T)$: given a value \emph{v} and a set of time points $T$, return the time point $t e T$ such that $|v-s(t)|$ is minimal.\\
\end{itemize}
The lookup operation can be efficiently performed by the circular array, while the neighbor operation takes advantage of the fact that the leaves of a $B^+$ tree are sorted and, unlike in a traditional $B^+$ tree, interconnected in both directions. \\
Since the system should be integrable in the TKCM algorithm, it needs to be able to handle the Data Set presented in Section \ref{sec:DS}. Therefore, the $B^+$ tree must be capable to deal with duplicate values, because the same temperature value in a streaming time series may occur several times. 

\section{Data Structures}
\label{sec:das}
To efficiently implement the above mentioned operations the system combines two data structures:
a circular array and a $B^+$ tree. In Figure \ref{fig:B+tree planned} the originally proposed data structure in \cite{BScT} is shown. The $B^+$ tree is connected with pointers to the circular array and vice versa. Further, the temperature values are used as keys in the $B^+$ tree. The size of the circular array is defined as $|W| + 1$ since an empty field is used to identify the current update position. The size $|W|$ and the order of the $B^+$ tree, which defines the size of the nodes, are parameters. Hence they can be changed.\\The circular array and the $B^+$ tree are characterized in the following. 

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
scale=0.7,
every node/.style={outer sep=0pt, transform shape, font=\scriptsize},
every matrix/.style={cells={scale=0.7}},
]
% root node
\xyshift{-20mm}{0mm}{\btreeinodethree{root}{18.6}{}{}};

%
% intermediate nodes
\xyshift{-50mm}{-15mm}{\btreeinodethree{n1}{17.3}{}{}{}}
\xyshift{ 10mm}{-15mm}{\btreeinodethree{n2}{19.2}{19.4}{19.7}}
%
% connecting root to intermediate level nodes
\foreach \x in {1,2} { \btreelink{root-\x}{n\x} }
%
% leaf nodes
\xyshift{-90mm}{-30mm}{\btreelnodethree{n11}{17.2}{}{}}
\xyshift{-60mm}{-30mm}{\btreelnodethree{n12}{17.3}{18.2}{18.3}}
\xyshift{-30mm}{-30mm}{\btreelnodethree{n21}{18.6}{18.8}{}}
\xyshift{ 0mm}{-30mm}{\btreelnodethree{n22}{19.2}{19.3}{}}
\xyshift{ 30mm}{-30mm}{\btreelnodethree{n23}{19.4}{}{}}
\xyshift{ 60mm}{-30mm}{\btreelnodethree{n24}{19.7}{19.7}{19.8}}
%
% connecting intermediate level to leaf nodes
\foreach \x in {1,2}     { \btreelink{n1-\x}{n1\x} }
\foreach \x in {1,2,3,4} { \btreelink{n2-\x}{n2\x} }
%
% leaf pointers
\draw[btlink] ([yshift=+3pt] n11-c.east) -- ([yshift=+3pt] n12-a.west);
\draw[btlink] ([yshift=-3pt] n12-a.west) -- ([yshift=-3pt] n11-c.east);
\draw[btlink] ([yshift=+3pt] n12-c.east) -- ([yshift=+3pt] n21-a.west);
\draw[btlink] ([yshift=-3pt] n21-a.west) -- ([yshift=-3pt] n12-c.east);
\draw[btlink] ([yshift=+3pt] n21-c.east) -- ([yshift=+3pt] n22-a.west);
\draw[btlink] ([yshift=-3pt] n22-a.west) -- ([yshift=-3pt] n21-c.east);
\draw[btlink] ([yshift=+3pt] n22-c.east) -- ([yshift=+3pt] n23-a.west);
\draw[btlink] ([yshift=-3pt] n23-a.west) -- ([yshift=-3pt] n22-c.east);
\draw[btlink] ([yshift=+3pt] n23-c.east) -- ([yshift=+3pt] n24-a.west);
\draw[btlink] ([yshift=-3pt] n24-a.west) -- ([yshift=-3pt] n23-c.east);
%
% circular array
\xyshift{-15mm}{-70mm}{
	\matrix [ampersand replacement=\&, outer sep=0pt, matrix anchor=north] (array) {
		\node[circularptr] (c1)  {}; \&
		\node[circularptr] (c2)  {}; \&
		\node[circularptr] (c3)  {}; \&
		\node[circularptr] (c4)  {}; \&
		\node[circularptr] (c5)  {}; \&
		\node[circularptr] (c6)  {}; \&
		\node[circularptr] (c7)  {}; \&
		\node[circularptr] (c8)  {}; \&
		\node[circularptr] (c9)  {}; \&
		\node[circularptr] (c10) {}; \&
		\node[circularptr] (c11) {}; \&
		\node[circularptr] (c12) {}; \&
		\node[circularptr] (c13) {}; \\
		%
		\node[circularval] (left) {14:15}; \&
		\node[circularval] {14:20}; \&
		\node[circularval] {14:25}; \&
		\node[circularval] {\phantom{14:25}}; \&
		\node[circularval] {13:30}; \&
		\node[circularval] {13:35}; \&
		\node[circularval] {13:40}; \&
		\node[circularval] {13:45}; \&
		\node[circularval] {13:50}; \&
		\node[circularval] {13:55}; \&
		\node[circularval] {14:00}; \&
		\node[circularval] {14:05}; \&
		\node[circularval] (right) {14:10}; \\
	};
}

%
% draw pointers between circular array and B+ tree
\path[btdlink] ([yshift=-2pt] c2.center)  edge[out=90,in=270] ([yshift=2pt] n22-2.center);
\path[btdlink] ([yshift=-2pt] c5.center)  edge[out=90,in=270] ([yshift=2pt] n22-1.center);
\path[btdlink] ([yshift=-2pt] c11.center) edge[out=90,in=270] ([yshift=2pt] n23-1.center);
\foreach \x in
{1,3,6,7,8,9,10,12,13}
{ \path[->] ([yshift=15pt] c\x.center) edge ([yshift=-2pt] c\x.center); }
\foreach \x in
{n11-1,n12-1,n12-2,n21-1,n21-2,n24-1,n24-2,n24-3}
{ \path[->] ([yshift=-15pt] \x.center) edge ([yshift=2pt] \x.center); }
%
% draw borders
\draw[densely dotted] (-11,1.5) rectangle (8,-5.5);
\draw[densely dotted] (-11,-6) rectangle (8,-10);
\node[anchor=south west] at (-11,1.5) {\Large $B^+$ tree};
\node[anchor=north west] at (-11,-10)  {\Large Circular array};
%
% curly brace
\draw [decorate,decoration={brace,mirror,amplitude=10pt}]
([yshift=-5pt] left.south west) -- ([yshift=-5pt] right.south east)
node [black,midway,yshift=-0.9cm] {\large size $|W|+1$};
\end{tikzpicture}
\vspace{2mm}
\caption{Proposed data structures in \cite{BScT}.}
\label{fig:B+tree planned}
\end{figure}

\subsection{Circular Array}
A circular array is used to store the time series data. The data is assorted by time. Further, the time interval is predefined e.g. every 5 minutes a new value arrives.\\The circular array presented in Figure \ref{fig:B+tree planned} has a size of $|W| + 1$. The empty field is used to identify the next update position. Hence, the position where the next arriving value is inserted. In addition to the time, the associated temperature value is stored in the $B^+$ tree and linked with a pointer.\\
But the circular array presented in the present thesis is slightly different. The value must not be associated by a pointer, but can also be directly stored in the circular array. Besides, instead of an empty field to identify the next update position, the position is stored in a variable and updated with every insertion. The circular array is shown in Figure \ref{fig:cat}
\begin{figure}[htbp]
	\centering
	\begin{tikzpicture}[
	scale=0.7,
	every node/.style={outer sep=0pt, transform shape, font=\scriptsize},
	every matrix/.style={cells={scale=0.7}},
	]

	% circular array
	\xyshift{-15mm}{-70mm}{
		\matrix [ampersand replacement=\&, outer sep=0pt, matrix anchor=north] (array) {
			\node[circularptr] (c1)  {}; \&
			\node[circularptr] (c2)  {}; \&
			\node[circularptr] (c3)  {}; \&
			\node[circularptr] (c4)  {}; \&
			\node[circularptr] (c5)  {}; \&
			\node[circularptr] (c6)  {}; \&
			\node[circularptr] (c7)  {}; \&
			\node[circularptr] (c8)  {}; \&
			\node[circularptr] (c9)  {}; \&
			\node[circularptr] (c10) {}; \&
			\node[circularptr] (c11) {}; \&
			\node[circularptr] (c12) {};  \\
			%
			\node[circularval] (left) {14:10}; \&
			\node[circularval] {14:15}; \&
			\node[circularval] {14:20}; \&
			\node[circularval] {14:25}; \&
			\node[circularval] {13:30}; \&
			\node[circularval] {13:35}; \&
			\node[circularval] {13:40}; \&
			\node[circularval] {13:45}; \&
			\node[circularval] {13:50}; \&
			\node[circularval] {13:55}; \&
			\node[circularval] {14:00}; \&
			\node[circularval] (right){14:05};\\
		};
	}
	% curly brace
	\draw [decorate,decoration={brace,mirror,amplitude=10pt}]
	([yshift=-5pt] left.south west) -- ([yshift=-5pt] right.south east)
	node [black,midway,yshift=-0.9cm] {\large size $|W|$};
	\end{tikzpicture}
	\vspace{2mm}
	\caption{Circular array without pointers to the $B^+$ tree.}
	\label{fig:cat}
\end{figure}

\subsection{$B^+$ Tree}
A $B^+$ tree is able to execute range queries very efficiently because the leaves of a $B^+$ tree are ordered and linked. To perform the \emph{neighbor}$(v,T)$ operation described in Section \ref{sec:Op} even better, the $B^+$ tree we use has its leaves linked in both directions. The Section \ref{structureBtree} describes the genaral structure of a $B^+$ tree. Further, the differences between the $B^+$ tree used in the present thesis and the $B^+$ tree presented in \cite{BTreeBook} are discussed. 

\subsubsection{The Structure of $B^+$ trees}
\label{structureBtree}
A $B^+$ tree is organized in bocks, as implied by its name. All paths from the root to a leaf have the same length, hence it is \emph{balanced}. There are three types of nodes that may exist in a $B^+$ tree: the root, interior nodes and leaves as you can see in Figure \ref{fig:cat}. The parameter \emph{n} determines the size of the blocks in the $B^+$ tree. Hence, the blocks can have maximum \emph{n} search-keys and \emph{n} + 1 pointers, pointing to its child nodes. Every block is between half full and completely full. What can appear in blocks is listed in the following: \\
\begin{itemize}  
	\item The keys in the leaves are sorted from left to right. 
	\item All pointers in a node point to the level below.
	\item The interior nodes use at least $[\frac{n+1}{2}]$ of its \emph{n} + 1 pointers. In the root at least 2 pointers must be used. 
	\item The first pointer in a node where the first key is \emph{K} points to a node and hence a part of the tree where the keys are less than \emph{K}. The second pointer points to a node where the keys are greater than or equal to \emph{K}, as shown in Figure \ref{lessOrEqual}. \\
\end{itemize}
The $B^+$ tree used for the implementation of the streaming time series data is slightly different to the  $B^+$ tree in \cite{BTreeBook}. The required properties are the following: \\
\begin{itemize}  
	\item The search-keys of the $B^+$ tree are the temperature values in the sliding window $W$.
	\item The leaves are linked in both directions to efficiently perform the \emph{neighbor}$(v,T)$ operation.
	\item The leaves are sorted by the temperature values.\\ 
\end{itemize}
Due to the weather conditions the temperature in the time window $W$ can occur several times. Therefore, the keys are not unique. Since the temperature values are used as search-keys, the $B^+$ tree must be able to handle duplicate values. Section \ref{allowDV} proposes different possibilities that allow to use duplicate values in a $B^+$ tree. 
\begin{figure}[H]
	\centering
	\begin{tikzpicture}[
	scale=0.7,
	every node/.style={outer sep=0pt, transform shape, font=\scriptsize},
	every matrix/.style={cells={scale=0.7}},
	]
	% root node
	
	\xyshift{-20mm}{0mm}{\btreeinodethree{root}{18.8}{}{}};
	
	%
	% intermediate nodes
	\xyshift{-50mm}{-15mm}{\btreeinodethree{n1}{17.3}{}{}{}}
	\xyshift{ 10mm}{-15mm}{\btreeinodethree{n2}{18.8}{19.4}{19.7}}
	
	%
	% connecting root to intermediate level nodes
	\foreach \x in {1,2} { \btreelink{root-\x}{n\x} }
	%
	
	\end{tikzpicture}
	\vspace{2mm}
	\caption{Left children keys < 18.8 and right children keys $\leq 18.8$ .}
	\label{lessOrEqual}
\end{figure}


\section{Allowing Duplicate Values}
\label{allowDV}
B+ trees are often 

\subsection{Methods Discussion}

\subsubsection{Add the time to the key}
If you would add the timestamps to keys would be unique. So if the case occurs where you have the same keys but in different leaves because the leaves were full and had therefore be splitted you could include the timestamps which divides the two different values to the parent. --> every key with timestamp smaller than the parents timestamp is in the left leaf and every key with timestamp bigger or equal is in the right one --> 
Problems: more memory needed. always check needed if there is a timestamps in the leaf
+ neighbor method would still work because you have the value and the set of time stamps 

\subsubsection{Add several times to the key}

The second was to have just one entry for each key, but the 'payload' associated with each key points to a different block of data, which is a linked list pointing to all instances of items that are having the same key


\subsubsection{Not change insertion but the search algorithm}
There are several different ways of handling duplicate keys. One way is use an unmodified insert algorithm which allows duplicate keys in blocks but is otherwise unchanged. The issue with a structure such as this is the search algorithm must be modified to take into account several corner cases which arise
For instance one of the invariants of a B+Tree may be violated in this structure. Specifically if there are many duplicate keys, a copy of one of the keys may be in a non-leaf block. However, the key may appear in blocks that which appear logically before the block which is pointed at by the key in the internal block. Thus the search algorithm must be modified to look in the previous blocks to the one suggested by the unmodified search algorithm. This will slow down the common case for search.There is another issue with this straight forward implementation, if there are many duplicate keys in the index, the index size may be taller than necessary. Consider a situation were for each unique key there are perhaps hundreds of duplicates, the index size will be proportional to the total number of keys in the main file, however, you only need to index an index on the unique keys. One of the files indexed in our program will be indexing has such characteristics to its data. It indexes strings (as the keys) with associated instances where those strings show up in our documents. There can be hundreds to thousands of instances of each unique string.
Therefore the approach I took was to store only the unique keys in the index, and have the duplicates captured in overflow blocks in the main file. An example of such a tree can be seen in figure 2. Consider key 6; there are 5 instances of this key in the tree. The tree is order 3, indicating the keys cannot all fit in one block. To handle this situation an overflow block is chained to the block which is indexed by the tree structure. The overflow block then points to the next relevant block in the tree.To create a structure such as this, the insert algorithm had to be modified. Like the previous version these modifications do not come without a cost, in particular the invariant which states all block must be at least half full has been relaxed. This is not true in this B+Tree, some blocks like the one containing key number 7, are not half full. This problem could be partially solved by using key rotations to balance the tree better. However, there are still corner cases where there would be a block which is under-full. One such corner case includes when a key falls between two keys which have overflow blocks. It must then be in a block by itself, since this B+Tree has the invariant which state that if a block is overflowing it can only contain one unique key. In the future we would like to implement key rotations to help partially alleviate the problem of under-full blocks.

The advantage of this approach to B+Trees with duplicate keys is the index size is small no matter how the ratio of duplicate keys to the total number of keys in the file. This property allows our searches to be conducted quicker. Since the overflow blocks are chained into the B+Tree structure we still have the property of being able to fast sequential scans. One consequence is we have defined all queries on our B+Trees to be range queries. This is fine because all of our queries were already range queries. In conclusion we relax the condition that all blocks must be at least half full to gain higher performance during search.

http://hackthology.com/lessons-learned-while-implementing-a-btree.html

\subsection{Book }

\chapter{Implementation}
\label{sec:Implementation}



\chapter{Evaluation}
\label{sec:Experimental}
\section{Experimental Setup}
\section{Results}
\section{Discussion}

\chapter{Summary and Conclusion}
\label{sec:Summary}
"The conclusion (10 to 12 per cent of the whole research thesis) does not only summarize the whole research thesis, but it also evaluates the results of the scientific inquiry. Do the results confirm or reject previously formulated hypotheses? The conclusion draws both theoretical and practical lessons that could be used in future analyses. These lessons are to be embedded as
2
recommendations for the research community and for policy-makers (note: policy relevance instead of policy prescriptive). In addition, the conclusion gives insights for further research."

\begin{thebibliography}{99}
	\bibliographystyle{alpha}
	
	\bibitem{BScT} K. Wellenzohn, M. Böhlen, A. Dignos, J. Gamper, and H. Mitterer: \emph{Continuous imputation of missing values in highly correlated streams of time series data}; Unpublished, 2016.
	
	\bibitem{OnlineAmnesicAppr} Themistoklis Palapanas, Michail Vlachos, Eamonn Keogh, Dimitrios Gunopulos, Wagner Truppel: \emph{Online Amnesic Approximation of Streaming Time Series}; University of California, Riverside, USA, 2004. \url{http://www.cs.ucr.edu/~eamonn/ICDM_2004.pdf}

	\bibitem{BTreeBook} Hector Garcia-Molina, Jeffrey D. Ullman, Jennifer Widom: \emph{Database Systems - The Complete Book}; ISBN 0-13-031995-3, 2002 by Prentice Hall
		
	



\end{thebibliography}






\end{document}
